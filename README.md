# llmakits

ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„Pythonå·¥å…·åŒ…ï¼Œç”¨äºç®€åŒ–å¤§è¯­è¨€æ¨¡å‹(LLM)çš„é›†æˆå’Œç®¡ç†ã€‚æ”¯æŒå¤šæ¨¡å‹è°ƒåº¦ã€æ•…éšœè½¬ç§»ã€è´Ÿè½½å‡è¡¡ç­‰åŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒOpenAIã€æ™ºè°±AIã€DashScopeã€ModelScopeç­‰å¤šä¸ªä¸»æµLLMå¹³å°
- ğŸ”„ **æ™ºèƒ½è°ƒåº¦**: å†…ç½®æ¨¡å‹æ•…éšœè½¬ç§»å’Œè´Ÿè½½å‡è¡¡æœºåˆ¶
- ğŸ”‘ **å¯†é’¥ç®¡ç†**: çµæ´»çš„APIå¯†é’¥é…ç½®å’Œç®¡ç†
- ğŸ“Š **æ¶ˆæ¯å¤„ç†**: å¼ºå¤§çš„æ¶ˆæ¯æ ¼å¼åŒ–ã€éªŒè¯å’Œæå–åŠŸèƒ½
- ğŸ›¡ï¸ **é”™è¯¯å¤„ç†**: å®Œå–„çš„é‡è¯•æœºåˆ¶å’Œå¼‚å¸¸å¤„ç†
- ğŸ“ **æµå¼è¾“å‡º**: æ”¯æŒæµå¼å“åº”å¤„ç†
- ğŸ¯ **ç”µå•†å·¥å…·**: å†…ç½®ç”µå•†åœºæ™¯ä¸“ç”¨å·¥å…·é›†

## å®‰è£…

```bash
pip install llmakits
```

æ›´æ–°ï¼š

```bash
pip install --upgrade llmakits
```

## å¿«é€Ÿå¼€å§‹

### 1. é…ç½®æ¨¡å‹å’ŒAPIå¯†é’¥

åˆ›å»ºé…ç½®æ–‡ä»¶ `config/models_config.yaml`ï¼š

```yaml
Models_config:
  my_models:
    - sdk_name: "openai"
      model_name: "gpt-3.5-turbo"
    - sdk_name: "zhipu"
      model_name: "glm-4-flash"
```

åˆ›å»ºé…ç½®æ–‡ä»¶ `config/keys_config.yaml`ï¼š

```yaml
openai:
  base_url: "https://api.openai.com/v1"
  api_keys:
    - "your-openai-api-key-1"
    - "your-openai-api-key-2"

zhipu:
  base_url: "https://open.bigmodel.cn/api/paas/v4"
  api_keys:
    - "your-zhipu-api-key"
```

### 2. åŠ è½½æ¨¡å‹

```python
from llmakits import load_models

# åŠ è½½é…ç½®å¥½çš„æ¨¡å‹
models = load_models('config/models_config.yaml', 'config/keys_config.yaml')

# è·å–æ¨¡å‹ç»„
my_models = models['my_models']
```

### 3. å‘é€æ¶ˆæ¯

#### ä½¿ç”¨æ–°çš„ ModelDispatcher ç±»ï¼ˆæ¨èï¼‰

```python
from llmakits.dispatcher import ModelDispatcher

# åˆ›å»ºè°ƒåº¦å™¨å®ä¾‹
dispatcher = ModelDispatcher()

# å‡†å¤‡æ¶ˆæ¯
message_info = {
    "system": "ä½ æ˜¯ä¸€ä¸ª helpful åŠ©æ‰‹",
    "user": "è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
}

# æ‰§è¡Œä»»åŠ¡
result, tokens = dispatcher.execute_task(message_info, my_models)
print(f"ç»“æœ: {result}")
print(f"ä½¿ç”¨tokenæ•°: {tokens}")

# æŒ‰éœ€è·å–æ¨¡å‹åˆ‡æ¢æ¬¡æ•°ï¼ˆä¸¤ç§æ–¹æ³•ï¼‰
switch_count = dispatcher.model_switch_count  # ç›´æ¥è®¿é—®å±æ€§
print(f"æ¨¡å‹åˆ‡æ¢æ¬¡æ•°: {switch_count}")

# æˆ–è€…ä½¿ç”¨æ–¹æ³•è·å–
switch_count = dispatcher.get_model_switch_count()
print(f"æ¨¡å‹åˆ‡æ¢æ¬¡æ•°: {switch_count}")

# å¦‚æœéœ€è¦æ‰§è¡Œå¤šä¸ªä»»åŠ¡ï¼Œå¯ä»¥é‡ç½®è®¡æ•°å™¨
dispatcher.reset_model_switch_count()
```

#### ä½¿ç”¨å…¼å®¹å‡½æ•°ï¼ˆæ—§ç‰ˆæœ¬ï¼‰

```python
from llmakits.dispatcher import execute_task

# å‡†å¤‡æ¶ˆæ¯
message_info = {
    "system": "ä½ æ˜¯ä¸€ä¸ª helpful åŠ©æ‰‹",
    "user": "è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
}

# æ‰§è¡Œä»»åŠ¡
result, tokens, switch_count = execute_task(message_info, my_models)
print(f"ç»“æœ: {result}")
print(f"ä½¿ç”¨tokenæ•°: {tokens}")
print(f"æ¨¡å‹åˆ‡æ¢æ¬¡æ•°: {switch_count}")
```

### 4. ç›´æ¥ä½¿ç”¨æ¨¡å‹å®¢æˆ·ç«¯

```python
from llmakits import BaseOpenai

# åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯
model = BaseOpenai(
    platform="openai",
    base_url="https://api.openai.com/v1",
    api_keys=["your-api-key"],
    model_name="gpt-3.5-turbo"
)

# å‘é€æ¶ˆæ¯
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ª helpful åŠ©æ‰‹"},
    {"role": "user", "content": "Hello!"}
]

result, tokens = model.send_message(messages)
print(f"å›å¤: {result}")
```

## é«˜çº§åŠŸèƒ½

### æ¶ˆæ¯å¤„ç†

```python
from llmakits.message import prepare_messages, extract_field, convert_to_json

# å‡†å¤‡æ¶ˆæ¯
messages = prepare_messages(system="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹", user="è¯·å¸®å¿™", assistant="å¥½çš„")

# æå–å¹¶è½¬æ¢ä¸ºJSON
json_str = '{"name": "test"} some text'
result = convert_to_json(json_str)

# æå–å­—æ®µ
field_value = extract_field(json_str, "name")
print(field_value)  # è¾“å‡º: test

# æå–å¤šä¸ªå­—æ®µ
name, age = extract_field(json_str, "name", "age")
print(name)  # è¾“å‡º: test
print(age)  # è¾“å‡º: None

```

### æ¨¡å‹è°ƒåº¦

#### ä½¿ç”¨ ModelDispatcher ç±»ï¼ˆæ¨èï¼‰

```python
from llmakits.dispatcher import ModelDispatcher

# åˆ›å»ºè°ƒåº¦å™¨å®ä¾‹
dispatcher = ModelDispatcher()

# å¸¦éªŒè¯å‡½æ•°çš„ä»»åŠ¡æ‰§è¡Œ
def validate_result(result):
    return "error" not in result.lower()

# æ‰§è¡Œä»»åŠ¡
result, tokens = dispatcher.execute_task(
    message_info={"user": "ç”Ÿæˆä¸€æ®µJSON"},
    llm_models=my_models,
    format_json=True,  # æ ¼å¼åŒ–ä¸ºJSON
    validate_func=validate_result  # éªŒè¯ç»“æœ
)

# è·å–æ¨¡å‹åˆ‡æ¢æ¬¡æ•°
switches = dispatcher.model_switch_count  # ç›´æ¥è®¿é—®å±æ€§
print(f"æ¨¡å‹åˆ‡æ¢æ¬¡æ•°: {switches}")
```

#### ä½¿ç”¨å…¼å®¹å‡½æ•°

```python
from llmakits.dispatcher import execute_task

# å¸¦éªŒè¯å‡½æ•°çš„ä»»åŠ¡æ‰§è¡Œ
def validate_result(result):
    return "error" not in result.lower()

result, tokens = execute_task(
    message_info={"user": "ç”Ÿæˆä¸€æ®µJSON"},
    llm_models=my_models,
    format_json=True,  # æ ¼å¼åŒ–ä¸ºJSON
    validate_func=validate_result  # éªŒè¯ç»“æœ
)
```

### ç”µå•†å·¥å…·

```python
from llmakits.e_commerce import ECommerceTools

# ä½¿ç”¨ç”µå•†ä¸“ç”¨å·¥å…·
ec_tools = ECommerceTools()
result = ec_tools.generate_product_description("ç”µå­äº§å“", "æ™ºèƒ½æ‰‹æœº")
```

## æ”¯æŒçš„æ¨¡å‹å¹³å°

- **OpenAI**: GPT-3.5, GPT-4 ç­‰ç³»åˆ—æ¨¡å‹
- **æ™ºè°±AI (Zhipu)**: GLM-4, GLM-3 ç­‰ç³»åˆ—æ¨¡å‹
- **DashScope**: é€šä¹‰åƒé—®ç³»åˆ—æ¨¡å‹
- **ModelScope**: å„ç§å¼€æºæ¨¡å‹å¦‚Qwenã€DeepSeekç­‰

## é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½® (`models_config.yaml`)

æŒ‰åŠŸèƒ½åˆ†ç»„é…ç½®æ¨¡å‹ï¼Œæ”¯æŒä¸ºä¸åŒåœºæ™¯é…ç½®ä¸åŒçš„æ¨¡å‹ç»„åˆï¼š

```yaml
Models_config:
  # æ ‡é¢˜ç”Ÿæˆä¸“ç”¨æ¨¡å‹ç»„
  generate_title:
    - sdk_name: "dashscope"
      model_name: "qwen3-max-preview"

  # ç¿»è¯‘ä¸“ç”¨æ¨¡å‹ç»„
  translate_box:
    - sdk_name: "modelscope"
      model_name: "Qwen/Qwen3-32B"
```

### å¯†é’¥é…ç½® (`keys_config.yaml`)

æ”¯æŒå¤šå¯†é’¥é…ç½®ï¼Œè‡ªåŠ¨åˆ‡æ¢å’Œè´Ÿè½½å‡è¡¡ï¼š

```yaml
platform_name:
  base_url: "api-endpoint-url"
  api_keys:
    - "api-key-1"
    - "api-key-2"
```

## é”™è¯¯å¤„ç†

llmakitså†…ç½®äº†å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼š

- **è‡ªåŠ¨é‡è¯•**: è¯·æ±‚å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š5æ¬¡
- **å¯†é’¥åˆ‡æ¢**: APIå¯†é’¥å¤±æ•ˆæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨å¯†é’¥
- **æ¨¡å‹åˆ‡æ¢**: å½“å‰æ¨¡å‹å¤±è´¥æ—¶è‡ªåŠ¨å°è¯•ä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹
- **å¼‚å¸¸æ•è·**: è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯å’Œå¤„ç†å»ºè®®

## è®¸å¯è¯

Apache 2.0 License
